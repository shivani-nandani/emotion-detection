{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import cv2\r\n",
    "import face_recognition\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "from PIL import Image\r\n",
    "import numpy as np\r\n",
    "from keras.models import load_model\r\n",
    "from fer import FER"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rohin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\r\n",
    "emotion_dict= {'Angry': 0, 'Sad': 5, 'Neutral': 4, 'Disgust': 1, 'Surprise': 6, 'Fear': 2, 'Happy': 3}\r\n",
    "model = load_model('my_model.hdf5')\r\n",
    "emo_detector = FER(mtcnn=True)\r\n",
    "\r\n",
    "video = cv2.VideoCapture(0)\r\n",
    "\r\n",
    "while True:\r\n",
    "    check, frame = video.read()\r\n",
    "    faces = face_cascade.detectMultiScale(frame, scaleFactor=1.1, minNeighbors=5)\r\n",
    "    for x,y,w,h in faces:\r\n",
    "        frame = cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 3)\r\n",
    "        face = frame[y:y+h, x:x+w]\r\n",
    "        face = cv2.resize(face,(48,48))\r\n",
    "        # face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\r\n",
    "        # face = np.reshape(face, (1, 48, 48, 3))\r\n",
    "        face = np.reshape(face, (48, 48, 3))\r\n",
    "\r\n",
    "        # predicted_class = np.argmax(model.predict(face))\r\n",
    "        # label_map = dict((v,k) for k,v in emotion_dict.items()) \r\n",
    "        # predicted_label = label_map[predicted_class]\r\n",
    "        dominant_emotion, emotion_score = emo_detector.top_emotion(face)\r\n",
    "\r\n",
    "        frame = cv2.putText(frame, dominant_emotion, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\r\n",
    "\r\n",
    "    \r\n",
    "    cv2.imshow(\"Emotion Detector\", frame)\r\n",
    "\r\n",
    "    key = cv2.waitKey(1)\r\n",
    "\r\n",
    "    if key == ord('q'):\r\n",
    "        break\r\n",
    "\r\n",
    "video.release()\r\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "metadata": {
   "interpreter": {
    "hash": "0ceb7df3098b8ddd2b0ae54ebbe153ee896dd252f19827a873e4983546c47608"
   }
  },
  "interpreter": {
   "hash": "31c2e02797b66426c2889ef59aa40ce35076b7e967fc017e70dba3520c68258b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}